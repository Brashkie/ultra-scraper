<div align="center">

# ğŸš€ Ultra Scraper

![Version](https://img.shields.io/badge/version-1.1.0-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Node](https://img.shields.io/badge/node-%3E%3D16.0.0-brightgreen.svg)
![TypeScript](https://img.shields.io/badge/TypeScript-5.9.3-blue.svg)
![Tests](https://img.shields.io/badge/tests-passing-success.svg)
![Coverage](https://img.shields.io/badge/coverage-67%25-yellow.svg)

### ğŸŒŸ Sistema profesional de web scraping con capacidades anti-bot, gestiÃ³n de concurrencia y extracciÃ³n inteligente

**Ultra Scraper** es un motor de scraping universal diseÃ±ado para manejar cualquier tipo de sitio web: HTTP, HTTPS, estÃ¡tico, dinÃ¡mico o protegido. Ofrece una API simple, rÃ¡pida y modular, ideal para bots, automatizaciÃ³n, anÃ¡lisis de datos y pipelines empresariales.

[InstalaciÃ³n](#-instalaciÃ³n) â€¢ [GuÃ­a RÃ¡pida](#-guÃ­a-rÃ¡pida) â€¢ [CaracterÃ­sticas](#-caracterÃ­sticas) â€¢ [Ejemplos](#-ejemplos-completos) â€¢ [API](#-api-reference)

</div>

---

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Novedades v1.1.0](#-novedades-v110)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [GuÃ­a RÃ¡pida](#-guÃ­a-rÃ¡pida)
- [Uso Avanzado](#-uso-avanzado)
- [CaracterÃ­sticas Avanzadas](#-caracterÃ­sticas-avanzadas)
  - [Browser Pool](#browser-pool)
  - [Sistema de Colas](#sistema-de-colas)
  - [Anti-Bot Detection](#anti-bot-detection)
  - [Rate Limiting](#rate-limiting)
  - [Load Balancing](#load-balancing)
  - [Concurrency Manager](#concurrency-manager)
- [Plugins](#-plugins)
- [Ejemplos Completos](#-ejemplos-completos)
- [API Reference](#-api-reference)
- [Testing](#-testing)
- [Troubleshooting](#-troubleshooting)
- [Changelog](#-changelog)
- [Contribuir](#-contribuir)
- [Roadmap](#-roadmap)
- [Licencia](#-licencia)

---

## âœ¨ CaracterÃ­sticas

<table>
<tr>
<td width="50%">

### ğŸŒ Versatilidad
- âœ… Soporte para **HTTP** y **HTTPS**
- âœ… Scraping de contenido **estÃ¡tico** y **dinÃ¡mico**
- âœ… Modo **headless** opcional (Playwright)
- âœ… Compatible con sitios protegidos
- âœ… TypeScript ready con IntelliSense completo

</td>
<td width="50%">

### âš¡ Rendimiento
- âœ… Parsers de **alto rendimiento** (Cheerio)
- âœ… **Auto-reintentos** con backoff exponencial
- âœ… RotaciÃ³n de **user-agents** y **proxys**
- âœ… Sistema de **plugins** extensible
- âœ… Pool automÃ¡tico de navegadores

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“Š ExtracciÃ³n de datos
- âœ… HTML, JSON, texto y atributos
- âœ… ImÃ¡genes, videos y archivos media
- âœ… Selectores CSS y XPath
- âœ… ExtracciÃ³n estructurada con schemas
- âœ… Transformaciones personalizadas

</td>
<td width="50%">

### ğŸ› ï¸ Facilidad de uso
- âœ… API simple e intuitiva
- âœ… Factory functions para inicio rÃ¡pido
- âœ… ConfiguraciÃ³n flexible
- âœ… DocumentaciÃ³n completa con 30+ ejemplos
- âœ… Manejo robusto de errores

</td>
</tr>
</table>

### ğŸš€ **CaracterÃ­sticas Profesionales v1.1.0**

<table>
<tr>
<td width="50%">

#### ğŸŒ Browser Pool Pro
- Pool automÃ¡tico de navegadores
- Auto-scaling inteligente
- Health monitoring en tiempo real
- RecuperaciÃ³n automÃ¡tica de crashes
- Estrategias: Round-robin, Least-used

#### ğŸ“‹ Sistema de Colas Avanzado
- 4 niveles de prioridad (CRITICAL â†’ LOW)
- Persistencia en disco automÃ¡tica
- Dead Letter Queue
- Scheduler con cron, interval, delayed
- MÃ©tricas en tiempo real

#### ğŸ›¡ï¸ Anti-Bot Detection PRO
- DetecciÃ³n de Cloudflare (v1/v2/Turnstile)
- Soporte CAPTCHA (reCAPTCHA, hCaptcha, FunCaptcha)
- Fingerprint management realista
- 13 tÃ©cnicas de evasiÃ³n stealth
- SimulaciÃ³n de comportamiento humano

</td>
<td width="50%">

#### âš¡ GestiÃ³n de Concurrencia
- ConcurrencyManager con adaptive scaling
- RateLimiter con 4 estrategias:
  - Fixed Window
  - Sliding Window
  - Token Bucket
  - Leaky Bucket
- LoadBalancer con 6 estrategias
- ThrottleManager inteligente
- Circuit Breaker pattern

#### ğŸ“ˆ Monitoring & Analytics
- PerformanceMonitor con mÃ©tricas P50/P95/P99
- PoolAnalytics con detecciÃ³n de tendencias
- QueueMetrics con throughput tracking
- ExportaciÃ³n de mÃ©tricas
- Alertas configurables

#### ğŸ”„ Resiliencia
- Retry strategies configurables
- Circuit breaker automÃ¡tico
- Health checks continuos
- Graceful degradation
- Backup automÃ¡tico de estado

</td>
</tr>
</table>

---

## ğŸ‰ Novedades v1.1.0

### **âœ¨ Agregado**
- ğŸ¯ **Browser Pool System** - GestiÃ³n automÃ¡tica de mÃºltiples navegadores con auto-scaling
- ğŸ“‹ **Advanced Queue System** - 4 niveles de prioridad, persistencia y dead letter queue
- ğŸ—“ï¸ **Queue Scheduler** - ProgramaciÃ³n de tareas con soporte cron completo
- ğŸ›¡ï¸ **Anti-Bot Detection PRO** - DetecciÃ³n y evasiÃ³n de Cloudflare, WAF, CAPTCHAs
- ğŸ­ **Fingerprint Management** - Perfiles de navegador realistas y rotaciÃ³n automÃ¡tica
- ğŸ‘¤ **Bot Behavior Simulator** - SimulaciÃ³n de movimientos de mouse, scroll, typing
- ğŸ”„ **Concurrency Manager** - Control avanzado con adaptive scaling
- â±ï¸ **Rate Limiter Advanced** - 4 estrategias de rate limiting profesionales
- âš–ï¸ **Load Balancer** - 6 estrategias de distribuciÃ³n de carga
- ğŸ”Œ **Circuit Breaker** - PatrÃ³n de resiliencia para servicios inestables
- ğŸ“Š **Monitoring System** - MÃ©tricas, analytics y alertas en tiempo real

### **ğŸ”§ Mejorado**
- TypeScript 5.9.3 con types completos y exports organizados
- Cobertura de tests mejorada al 67%
- DocumentaciÃ³n extensa con 30+ ejemplos prÃ¡cticos
- CI/CD configurado con GitHub Actions
- Performance optimizado para alto volumen

### **ğŸ› Corregido**
- 66 errores de compilaciÃ³n TypeScript resueltos
- Problemas de tipos DOM en cÃ³digo de navegador
- Conflictos de nombres en tipos exportados
- Memory leaks en pools de larga duraciÃ³n
- Race conditions en queue system

---

## ğŸ“¦ InstalaciÃ³n
```bash
# InstalaciÃ³n completa (recomendada)
npm install ultra-scraper

# Instalar navegadores de Playwright (solo si usarÃ¡s scraping dinÃ¡mico)
npx playwright install chromium
```

### Requisitos
- **Node.js** >= 16.0.0
- **npm** >= 7.0.0

**Nota:** Playwright se instala automÃ¡ticamente. Los navegadores son opcionales si solo usas scraping HTTP.

---

## ğŸš€ GuÃ­a RÃ¡pida

### Scraping Simple
```typescript
import { createScraper } from "ultra-scraper";

const scraper = createScraper();

// Obtener contenido de una pÃ¡gina
const response = await scraper.get("https://example.com");

console.log(response.html);     // HTML completo
console.log(response.status);   // 200
console.log(response.headers);  // Headers HTTP
console.log(response.responseTime); // 245ms
```

### Scraping con Selectores CSS
```typescript
import { createScraper } from "ultra-scraper";

const scraper = createScraper();

// Extraer elementos especÃ­ficos
const $ = await scraper.query("https://example.com", "h1");
console.log($.text());  // Texto del h1

// MÃºltiples elementos
const $ = await scraper.query("https://example.com", "a");
$("a").each((i, elem) => {
  console.log($(elem).attr("href"));
});
```

### ExtracciÃ³n Estructurada
```typescript
const scraper = createScraper();

const products = await scraper.extract("https://shop.com/products", {
  selector: ".product",
  fields: {
    title: { selector: ".title", attr: "text" },
    price: { 
      selector: ".price", 
      attr: "text",
      transform: (val) => parseFloat(val.replace('$', ''))
    },
    image: { selector: "img", attr: "src" },
    link: { selector: "a", attr: "href" }
  },
  limit: 50
});

console.log(products);
// [
//   { title: "Product 1", price: 29.99, image: "...", link: "..." },
//   { title: "Product 2", price: 49.99, image: "...", link: "..." }
// ]
```

---

## ğŸ” Uso Avanzado

### Scraping DinÃ¡mico (SPAs)

Para sitios web que cargan contenido con JavaScript:
```typescript
import { createScraper } from "ultra-scraper";

const scraper = createScraper({
  dynamic: true,          // Habilita navegador headless
  retries: 3,             // NÃºmero de reintentos
  timeout: 12000,         // Timeout en ms
  waitForSelector: ".content"  // Esperar elemento especÃ­fico
});

const $ = await scraper.query("https://spa-website.com", ".dynamic-content");
console.log($.text());
```

### ConfiguraciÃ³n Completa de Opciones
```typescript
const scraper = createScraper({
  // NavegaciÃ³n
  dynamic: false,           // Usar navegador headless
  userAgent: "custom-ua",   // User-agent personalizado
  timeout: 10000,           // Timeout en milisegundos
  
  // Reintentos
  retries: 3,               // NÃºmero de reintentos
  retryDelay: 1000,         // Delay entre reintentos (ms)
  
  // Headers personalizados
  headers: {
    "Accept-Language": "es-ES",
    "Referer": "https://google.com"
  },
  
  // Proxy
  proxy: "http://proxy:8080",
  
  // Espera dinÃ¡mica
  waitForSelector: ".loaded",
  waitTime: 2000
});
```

---

## ğŸ“ CaracterÃ­sticas Avanzadas

### Browser Pool

GestiÃ³n automÃ¡tica de mÃºltiples navegadores para scraping paralelo eficiente:
```typescript
import { createBrowserPool } from 'ultra-scraper';

const pool = createBrowserPool(10); // Max 10 navegadores
await pool.initialize();

// Adquirir pÃ¡gina del pool
const page = await pool.acquirePage();

try {
  await page.goto('https://example.com');
  const content = await page.content();
  
  // Tu lÃ³gica de scraping...
  
} finally {
  // Liberar pÃ¡gina de vuelta al pool
  await pool.releasePage(page);
}

// MÃ©tricas en tiempo real
const metrics = pool.getMetrics();
console.log(`Navegadores activos: ${metrics.browserCount}`);
console.log(`PÃ¡ginas totales: ${metrics.totalPages}`);
console.log(`Requests exitosos: ${metrics.successfulRequests}`);

// Cerrar pool cuando termines
await pool.close();
```

**ConfiguraciÃ³n Avanzada:**
```typescript
import { BrowserPool } from 'ultra-scraper';

const pool = new BrowserPool({
  minBrowsers: 2,
  maxBrowsers: 10,
  maxPagesPerBrowser: 5,
  browserType: 'chromium',
  autoScale: true,
  scaleUpThreshold: 0.8,
  scaleDownThreshold: 0.2,
  launchOptions: {
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox']
  }
});
```

### Sistema de Colas

GestiÃ³n avanzada de tareas con prioridades y persistencia:
```typescript
import { createQueue, TaskPriority } from 'ultra-scraper';

const queue = createQueue(5); // 5 tareas concurrentes

// Agregar tarea con prioridad ALTA
await queue.add({
  id: 'scrape-homepage',
  priority: TaskPriority.HIGH,
  execute: async () => {
    const scraper = createScraper();
    return await scraper.get('https://example.com');
  }
});

// Agregar tarea CRÃTICA (se ejecuta primero)
await queue.add({
  id: 'scrape-urgent',
  priority: TaskPriority.CRITICAL,
  execute: async () => {
    return await scraper.get('https://urgent.example.com');
  }
});

// Agregar mÃºltiples tareas con delay
const urls = ['https://example.com/page1', 'https://example.com/page2'];
const tasks = urls.map(url => ({
  id: `scrape-${url}`,
  execute: async () => await scraper.get(url)
}));

await queue.addBatch(tasks, 500); // 500ms de delay entre tareas

// Eventos
queue.on('taskCompleted', (event) => {
  console.log(`âœ… Completado: ${event.taskId} en ${event.duration}ms`);
});

queue.on('taskError', (event) => {
  console.error(`âŒ Error: ${event.taskId}`, event.error);
});

// MÃ©tricas
const metrics = queue.getMetrics();
console.log(`Procesadas: ${metrics.totalProcessed}`);
console.log(`Fallidas: ${metrics.totalFailed}`);
console.log(`Tasa de Ã©xito: ${(1 - metrics.totalFailed / metrics.totalProcessed) * 100}%`);
```

**Queue Scheduler - Tareas Programadas:**
```typescript
import { QueueScheduler, CronBuilder } from 'ultra-scraper';

const scheduler = new QueueScheduler(queue);

// Ejecutar cada 5 minutos
scheduler.schedule(
  'periodic-scrape',
  'Scraping periÃ³dico de precios',
  () => ({
    id: `price-check-${Date.now()}`,
    execute: async () => await scraper.extract('https://shop.com/products', schema)
  }),
  {
    type: 'interval',
    interval: 5 * 60 * 1000
  }
);

// Todos los dÃ­as a las 2 AM
scheduler.schedule(
  'daily-scrape',
  'Scraping diario',
  () => ({ execute: async () => await fullSiteScrape() }),
  {
    type: 'cron',
    cronExpression: CronBuilder.everyDay(2, 0)
  }
);

// Horario de oficina - Lunes a Viernes, 9 AM a 5 PM
scheduler.schedule(
  'business-hours-scrape',
  'Scraping en horario laboral',
  () => ({ execute: async () => await scraper.get('https://business.com') }),
  {
    type: 'cron',
    cronExpression: '0 9-17 * * 1-5'
  }
);
```

### Anti-Bot Detection

DetecciÃ³n y evasiÃ³n automÃ¡tica de protecciones anti-bot:
```typescript
import { 
  AntiBotDetector, 
  StealthMode, 
  BotBehaviorSimulator 
} from 'ultra-scraper';
import { chromium } from 'playwright';

// ConfiguraciÃ³n completa
const detector = new AntiBotDetector({
  enableAutoDetection: true,
  enableAutoBypass: true,
  
  cloudflare: {
    enabled: true,
    waitForChallenge: true,
    maxWaitTime: 30000
  },
  
  captcha: {
    enabled: true,
    autoSolve: true,
    provider: '2captcha',
    apiKey: 'YOUR_2CAPTCHA_KEY'
  },
  
  fingerprint: {
    enabled: true,
    rotateOnBlock: true
  },
  
  humanBehavior: {
    enabled: true,
    mouseMovements: true,
    randomScrolling: true
  },
  
  stealth: {
    enabled: true,
    hideWebdriver: true
  }
});

const browser = await chromium.launch({ headless: true });
const page = await browser.newPage();

// Aplicar stealth mode
const stealthMode = new StealthMode({ enabled: true });
await stealthMode.apply(page);

// Navegar con comportamiento humano
const behaviorSim = new BotBehaviorSimulator({ enabled: true });
await page.goto('https://protected-site.com');

// Detectar bloqueos
const block = await detector.detectBlock(page);
if (block) {
  console.log(`Bloqueado: ${block.type}`);
}

// Simular comportamiento humano
await behaviorSim.simulatePageView(page);
await behaviorSim.simulateClick(page, '.button');
```

### Rate Limiting

Control preciso de velocidad de requests:
```typescript
import { RateLimiter } from 'ultra-scraper';

// Estrategia Token Bucket
const limiter = new RateLimiter({
  requestsPerSecond: 10,
  burst: 20,
  strategy: 'token-bucket',
  adaptive: {
    enabled: true,
    targetErrorRate: 0.1,
    increaseStep: 1,
    decreaseStep: 2
  }
});

// Uso
await limiter.acquire();
const response = await fetch('https://api.example.com/data');
if (response.ok) limiter.recordSuccess();
else limiter.recordError();

// EstadÃ­sticas
const stats = limiter.getStats();
console.log(`Tasa actual: ${stats.currentRate} req/s`);
```

### Load Balancing

DistribuciÃ³n inteligente de carga:
```typescript
import { LoadBalancer } from 'ultra-scraper';

const balancer = new LoadBalancer({
  strategy: 'least-connections',
  healthCheck: { enabled: true, interval: 10000 }
});

// Agregar targets (proxies, mirrors, etc.)
balancer.addTarget({
  id: 'proxy-1',
  url: 'http://proxy1.com:8080',
  weight: 2
});

// Ejecutar con balanceo automÃ¡tico
const result = await balancer.executeRequest(
  async (target) => {
    const scraper = createScraper({ proxy: target.url });
    return await scraper.get('https://target-site.com');
  }
);

// EstadÃ­sticas
const stats = balancer.getAllStats();
console.log(`Tasa de Ã©xito: ${stats.successRate * 100}%`);
```

### Concurrency Manager

GestiÃ³n inteligente con auto-scaling:
```typescript
import { ConcurrencyManager } from 'ultra-scraper';

const manager = new ConcurrencyManager({
  maxConcurrent: 10,
  adaptiveScaling: {
    enabled: true,
    minConcurrent: 2,
    maxConcurrent: 50
  }
});

// Ejecutar mÃºltiples tareas
const urls = Array.from({ length: 100 }, (_, i) => `https://example.com/page${i}`);

const results = await manager.executeMany(
  urls.map((url, i) => ({
    id: `scrape-${i}`,
    execute: async () => await scraper.get(url)
  }))
);

// Eventos de scaling
manager.on('scaledUp', (event) => {
  console.log(`ğŸ“ˆ Scaled to ${event.newConcurrency}`);
});

// MÃ©tricas
const metrics = manager.getMetrics();
console.log(`Tasa de Ã©xito: ${metrics.successRate * 100}%`);
```

---

## ğŸ§© Plugins

### Plugins Incluidos

#### RotaciÃ³n de Proxys
```typescript
import { createScraper } from "ultra-scraper";
import { proxyRotation } from "ultra-scraper/plugins";

const scraper = createScraper();

scraper.use(proxyRotation({
  proxies: [
    "http://proxy1.com:8080",
    "http://proxy2.com:8080",
    "http://proxy3.com:8080"
  ],
  rotateOnError: true
}));

// Los proxies se rotarÃ¡n automÃ¡ticamente
await scraper.get("https://example.com");
```

#### User-Agent Aleatorio
```typescript
import { randomUserAgent } from "ultra-scraper/plugins";

scraper.use(randomUserAgent({
  browsers: ['chrome', 'firefox', 'safari'],
  devices: ['desktop', 'mobile']
}));
```

#### Rate Limit
```typescript
import { rateLimit } from "ultra-scraper/plugins";

scraper.use(rateLimit({
  maxRequests: 5,
  windowMs: 1000 // 5 requests por segundo
}));
```

#### Retry AutomÃ¡tico
```typescript
import { retry } from "ultra-scraper/plugins";

scraper.use(retry({
  maxRetries: 3,
  retryDelay: 1000,
  exponentialBackoff: true,
  retryableErrors: ['ECONNRESET', 'ETIMEDOUT']
}));
```

### Plugin Personalizado
```typescript
import { ScraperPlugin } from "ultra-scraper";

const customPlugin: ScraperPlugin = {
  name: "custom-logger",
  
  beforeRequest: async (config) => {
    console.log(`ğŸš€ Request to: ${config.url}`);
    config.headers = { ...config.headers, 'X-Custom': 'value' };
    return config;
  },
  
  afterRequest: async (response) => {
    console.log(`âœ… Response: ${response.status}`);
    return response;
  },
  
  onError: async (error) => {
    console.error(`âŒ Error:`, error.message);
  }
};

scraper.use(customPlugin);
```

---

## ğŸ’¡ Ejemplos Completos

### E-commerce Price Monitor
```typescript
import { createScraper, createQueue, QueueScheduler, CronBuilder } from 'ultra-scraper';

class PriceMonitor {
  private scraper = createScraper({ dynamic: true });
  private queue = createQueue(5);
  private scheduler = new QueueScheduler(this.queue);

  constructor() {
    this.setupEventHandlers();
  }

  setupEventHandlers() {
    this.queue.on('taskCompleted', (event) => {
      if (event.result.priceChanged) {
        this.sendAlert(event.result);
      }
    });
  }

  async monitorProduct(url: string, targetPrice: number) {
    this.scheduler.schedule(
      `monitor-${url}`,
      `Price monitoring`,
      () => ({
        execute: async () => {
          const [product] = await this.scraper.extract(url, {
            selector: '.product',
            fields: {
              name: { selector: '.title', attr: 'text' },
              price: { 
                selector: '.price', 
                attr: 'text',
                transform: (v) => parseFloat(v.replace(/[$,]/g, ''))
              }
            }
          });

          return {
            product: product.name,
            currentPrice: product.price,
            priceChanged: product.price <= targetPrice
          };
        }
      }),
      {
        type: 'cron',
        cronExpression: CronBuilder.custom('0', '*', '*', '*', '*') // Cada hora
      }
    );
  }

  sendAlert(data: any) {
    console.log('ğŸ”” PRICE ALERT!');
    console.log(`${data.product}: $${data.currentPrice}`);
  }

  async start() {
    await this.monitorProduct('https://shop.com/laptop', 999);
    await this.monitorProduct('https://shop.com/phone', 699);
    console.log('ğŸ“Š Price monitor started!');
  }
}

const monitor = new PriceMonitor();
await monitor.start();
```

### News Aggregator con Load Balancing
```typescript
import { createScraper, LoadBalancer, RateLimiter } from 'ultra-scraper';

class NewsAggregator {
  private scraper = createScraper();
  private balancer = new LoadBalancer({ strategy: 'least-response-time' });
  private limiter = new RateLimiter({ requestsPerSecond: 5 });

  constructor() {
    // Agregar proxies
    ['proxy1', 'proxy2', 'proxy3'].forEach((name, i) => {
      this.balancer.addTarget({
        id: name,
        url: `http://${name}.example.com:8080`,
        weight: i + 1
      });
    });
  }

  async scrapeSource(url: string, schema: any) {
    await this.limiter.acquire();
    
    return await this.balancer.executeRequest(async (target) => {
      const scraper = createScraper({ proxy: target.url });
      return await scraper.extract(url, schema);
    });
  }

  async aggregateNews() {
    const sources = [
      { name: 'TechNews', url: 'https://tech.example.com', schema: {...} },
      { name: 'Business', url: 'https://business.example.com', schema: {...} }
    ];

    const allArticles = [];

    for (const source of sources) {
      try {
        const articles = await this.scrapeSource(source.url, source.schema);
        allArticles.push(...articles);
        console.log(`âœ… ${source.name}: ${articles.length} articles`);
      } catch (error) {
        console.error(`âŒ ${source.name}:`, error);
      }
    }

    return allArticles;
  }
}

const aggregator = new NewsAggregator();
const articles = await aggregator.aggregateNews();
console.log(`Total: ${articles.length} articles`);
```

---

## ğŸ“– API Reference

### Factory Functions

| FunciÃ³n | DescripciÃ³n | Ejemplo |
|---------|-------------|---------|
| `createScraper(options?)` | Crea instancia de Scraper | `createScraper({ dynamic: true })` |
| `createQueue(concurrency?)` | Crea TaskQueue | `createQueue(5)` |
| `createBrowserPool(max?)` | Crea BrowserPool | `createBrowserPool(10)` |
| `createExponentialBackoff()` | Crea BackoffStrategy | `createExponentialBackoff(1000, 30000)` |
| `createCircuitBreaker()` | Crea CircuitBreaker | `createCircuitBreaker(5, 60000)` |
| `createRetryStrategy()` | Crea RetryStrategy | `createRetryStrategy(3)` |

### Core Classes

#### Scraper

| MÃ©todo | DescripciÃ³n | Retorna |
|--------|-------------|---------|
| `get(url, options?)` | Obtiene HTML | `Promise<ScraperResponse>` |
| `query(url, selector, options?)` | Obtiene Cheerio API | `Promise<CheerioAPI>` |
| `extract(url, schema, options?)` | Extrae datos estructurados | `Promise<any[]>` |
| `use(plugin)` | Registra un plugin | `void` |

#### BrowserPool

| MÃ©todo | DescripciÃ³n |
|--------|-------------|
| `initialize()` | Inicializa el pool |
| `acquirePage()` | Obtiene pÃ¡gina |
| `releasePage(page)` | Libera pÃ¡gina |
| `getMetrics()` | Obtiene mÃ©tricas |
| `close()` | Cierra el pool |

#### TaskQueue

| MÃ©todo | DescripciÃ³n |
|--------|-------------|
| `add(task)` | Agrega tarea |
| `addBatch(tasks, delay?)` | Agrega mÃºltiples |
| `pause()` | Pausa procesamiento |
| `resume()` | Reanuda |
| `getMetrics()` | Obtiene mÃ©tricas |
| `shutdown()` | Apaga gracefully |

---

## ğŸ§ª Testing
```bash
# Ejecutar tests
npm test

# Tests en modo watch
npm run test:watch

# Cobertura de cÃ³digo
npm run test:coverage

# Tests especÃ­ficos
npm test -- --grep "BrowserPool"
```

---

## ğŸ› Troubleshooting

### Error: "playwright not installed"
```bash
npm install playwright
npx playwright install chromium
```

### Rate limit exceeded
```typescript
const scraper = createScraper({
  timeout: 60000,
  retries: 5
});
```

### Cloudflare Challenge no resuelve
```typescript
const detector = new AntiBotDetector({
  cloudflare: {
    maxWaitTime: 60000 // Aumentar a 60s
  }
});
```

### Memory leaks
```typescript
const pool = createBrowserPool(5);
try {
  const page = await pool.acquirePage();
  // ... tu cÃ³digo
} finally {
  await pool.releasePage(page); // Siempre liberar
}
```

---

## ğŸ“ Changelog

### v1.1.0 (2025-12-01) - MAJOR UPDATE

#### ğŸ‰ Nuevas CaracterÃ­sticas
- âœ… Browser Pool System con auto-scaling
- âœ… Advanced Queue System (4 prioridades, persistencia)
- âœ… Anti-Bot Detection PRO (Cloudflare, CAPTCHA)
- âœ… Concurrency Manager con adaptive scaling
- âœ… Rate Limiter (4 estrategias)
- âœ… Load Balancer (6 estrategias)
- âœ… Circuit Breaker pattern
- âœ… Monitoring & Analytics

#### ğŸ”§ Mejoras
- TypeScript 5.9.3
- Tests 67% coverage
- CI/CD GitHub Actions
- 30+ ejemplos

#### ğŸ› Correcciones
- 66 errores TypeScript
- Tipos DOM
- Memory leaks
- Race conditions

### v1.0.0 (2025-11-20)
- Primera versiÃ³n estable

---

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea tu branch (`git checkout -b feature/AmazingFeature`)
3. Commit (`git commit -m 'Add: nueva caracterÃ­stica'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Abre Pull Request

### Desarrollo Local
```bash
git clone https://github.com/Brashkie/ultra-scraper.git
cd ultra-scraper
npm install
npm run build
npm test
```

---

## ğŸ“ Roadmap

- [ ] Soporte para WebSockets
- [ ] IntegraciÃ³n con bases de datos
- [ ] Cache inteligente de respuestas
- [ ] CAPTCHA solving con IA
- [ ] CLI para scraping desde terminal
- [ ] Dashboard de monitoreo web
- [ ] IntegraciÃ³n con LangChain
- [ ] Scraping de archivos PDF
- [ ] API REST para scraping remoto

---

## ğŸ“„ Licencia

Apache-2.0 License - ver [LICENSE](LICENSE)

Copyright Â© 2025 [Hepein Oficial](https://github.com/Brashkie)

---

## ğŸ™ Agradecimientos

- [Playwright](https://playwright.dev/) - Browser automation
- [Cheerio](https://cheerio.js.org/) - HTML parsing
- [Axios](https://axios-http.com/) - HTTP client
- Comunidad open source

---

## ğŸ“ Soporte

- ğŸ“§ Email: support@ultra-scraper.com
- ğŸ› Issues: [GitHub Issues](https://github.com/Brashkie/ultra-scraper/issues)
- ğŸ“– Docs: [Documentation](https://github.com/Brashkie/ultra-scraper/wiki)

---

## ğŸ”— Enlaces

- [NPM Package](https://www.npmjs.com/package/ultra-scraper)
- [GitHub](https://github.com/Brashkie/ultra-scraper)
- [Changelog](https://github.com/Brashkie/ultra-scraper/releases)
- [Contributing Guide](CONTRIBUTING.md)

---

<div align="center">

**[â¬† Volver arriba](#-ultra-scraper)**

Hecho con â¤ï¸ por [Hepein Oficial](https://github.com/Brashkie)

â­ Si te gusta este proyecto, Â¡dale una estrella en GitHub!

</div>